{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Boundaries\n",
    "\n",
    "**BOUNDARY:** <br>\n",
    "**A LINE**\n",
    "\n",
    "$ w_1x_1 + w_2x_2 + b = 0 $ <br>\n",
    "$ W_x + b = 0 $ <br>\n",
    "$ W = (w_1, w_2) $ <br>\n",
    "$ x = (x_1, x_2) $ <br>\n",
    "$ y = label: 0 $ OR $ 1 $ <br>\n",
    "\n",
    "**PREDICTION:**\n",
    "\n",
    "\\begin{equation}\n",
    "  \\widehat{y} = \\begin{cases}\n",
    "    1, & \\text{if $Wx + b >= 0$}.\\\\\n",
    "    0, & \\text{if $Wx = b < 0$}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "# 2. Higher Dimensions\n",
    "\n",
    "n-dimensional space :: $ x_1, x_2 ... x_n $\n",
    "\n",
    "**BOUNDARY:** <br>\n",
    "n-1 dimensional hyperplane\n",
    "\n",
    "$ w_1x_1 + w_2x_2 + w_3x_3 ... w_nx_n + b = 0 $ <br>\n",
    "$ W_x + b = 0 $ <br>\n",
    "$ W = (w_1, w_2, ... w_n) $ <br>\n",
    "$ x = (x_1, x_2, ... x_n) $ <br>\n",
    "$ y = label: 0 $ OR $ 1 $ <br>\n",
    "\n",
    "**PREDICTION:**\n",
    "\n",
    "\\begin{equation}\n",
    "  \\widehat{y} = \\begin{cases}\n",
    "    1, & \\text{if $Wx + b >= 0$}.\\\\\n",
    "    0, & \\text{if $Wx = b < 0$}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "# 3. Perceptrons\n",
    "\n",
    "1. Getting inputes from one or more nodes \n",
    "2. Sending them collectively to proess\n",
    "3. Categorize outcome using stepfunction\n",
    "4. Delier outcome\n",
    "\n",
    "\n",
    "# 4. Why \"Neural Networks\"?\n",
    "\n",
    "The reason it's called **Neural Networks** is because it is similar to getting input via dendrite, processing it, and sending it out via axon as our neuros works.\n",
    "\n",
    "# 5. Perceptrons as Logical Operators\n",
    "\n",
    "* AND Perceptron\n",
    "* OR Perceptron\n",
    "* NOT Perceptron\n",
    "* XOR Perceptron\n",
    "\n",
    "# 6. Perceptron Trick\n",
    "\n",
    "1. Negative coordinate in Positive Area => Subtraction\n",
    "2. Positive coordinate in Negative Area => Addition\n",
    "\n",
    "# 7. Perceptron Algorithm\n",
    "\n",
    "1. Start with random weights:\n",
    "$ w_1, ... w_n, b $\n",
    "\n",
    "2. For every misclassified point ($ X_1, ... X_n $):\n",
    "\n",
    "    2.1 If prediction = 0:\n",
    "        * For i = 1 ... n:\n",
    "            * Change $ W_i +AX_i $\n",
    "        * Change b to b + A\n",
    "    2.2 If prediction = 1:\n",
    "        * For i = 1 ... n:\n",
    "            * Change $ W_i - Ax_i $\n",
    "        * Change b to b - A\n",
    "  \n",
    "# 8. Error Functions\n",
    "\n",
    "* Error Function = Distance\n",
    "\n",
    "# 9. Log-loss Error Function\n",
    "\n",
    "## Gradient Descent Concept\n",
    "\n",
    "1. Needs to be continous - Needs to be **Continuous** instead of Discrete. Continuous approach means you take a look at several directions of possible distance of how much of an error you can decrease and then take the largest possible distance to descrease error.\n",
    "\n",
    "2. Needs to be differentiable\n",
    "\n",
    "# 10. Discrete vs Continuous Predictions\n",
    "\n",
    "The prediction is basically the answer we get from the algorithm. A discrete answer will be of the form yes and no, whereas continuous answer will be of number, normally between zero and one which we'll consider a probability - The key is to change from **Discrete function to Continuous function**, or from **Step Function** to **Sigmoid Function**.\n",
    "\n",
    "Sigmoid Function is simply a function which for large positive numbers will give us values very close to one. For large negative numbers will give us values very close to zero. For the values close to zero, it will give you values that are close to 0.5.\n",
    "\n",
    "# 11. Multi-Class Classification and Softmax\n",
    "\n",
    "## The Softmax Function\n",
    "\n",
    "Equivalent of the sigmoid activation function, but when the problem has 3 or more classes.\n",
    "\n",
    "# 12. One-Hot Encoding\n",
    "\n",
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. It is essentially a dataset with number of columns same as number of variables where identifies which variable each element is by specifying binary value 0 or 1 to each column.\n",
    "\n",
    "# 13. Maximum Likelihood\n",
    "\n",
    "# 14. Maximizing Probabilities & Cross-Entropy\n",
    "\n",
    "# 15. Multi-Class Cross Entropy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
